# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wcUJZxCMs35KZtOGqTLdkieFjHfjxYcp
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow import keras
from tensorflow.keras import layers as Layer
from tensorflow.keras.metrics import Recall, Precision, BinaryAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy
from tensorflow.keras.regularizers import L1L2
from timeit import default_timer as timer

class TimingCallback(keras.callbacks.Callback):
    def on_train_begin(self, logs=None):
        print("Starting training")
        self.starttime = timer()

    def on_train_end(self, logs=None):
        print("End of training, took {} seconds".format(timer()-self.starttime))

    def on_epoch_end(self, epoch, logs=None):
        if epoch % 10 == 0:
          print("Epoch is {} and {} seconds passed".format(epoch, timer()-self.starttime))


def multi_model(drop_rate = 0.1,check_path = 'model2.h5'):
  model = Sequential()
  model.add(Layer.Dense(12, activation = 'relu'))
  model.add(Layer.Dropout(0.1))

  model.add(Layer.Dense(4, activation = 'relu'))
  model.add(Layer.Dropout(0.1))


  model.add(Layer.Dense(6, activation = 'softmax'))

  metrics = [
      SparseCategoricalAccuracy(name = 'accuracy') # Accuracy를 사용 안 하는 이유는 Accuracy가 이상하세 나왔기 때문.
  ]

  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
                 loss="sparse_categorical_crossentropy",
                 metrics=metrics)

  callback = [
    tf.keras.callbacks.EarlyStopping(monitor="loss",min_delta = 0.001, patience=100),
    tf.keras.callbacks.ModelCheckpoint(filepath=check_path, save_weights_only=True, monitor='loss', mode='min', save_best_only=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=100, min_lr=0.01),
    TimingCallback()
]

  return model

def train_model(model, x_train, y_train, epochs = 1000,
                batch_size=1024, validation_split=0.1):
  """Train the model by feeding it data."""

  history = model.fit(x=x_train, y=y_train, batch_size=batch_size,
                      epochs=epochs, shuffle=True,
                      validation_split=validation_split)

  return history